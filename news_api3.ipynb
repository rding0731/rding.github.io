{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hideCode": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "API_ROOT = 'http://api.nytimes.com/svc/search/v2/articlesearch.'\n",
    "\n",
    "API_SIGNUP_PAGE = 'http://developer.nytimes.com/docs/reference/keys'\n",
    "\n",
    "\n",
    "class NoAPIKeyException(Exception):\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "\n",
    "    def __str__(self):\n",
    "        return repr(self.value)\n",
    "\n",
    "\n",
    "class articleAPI(object):\n",
    "    def __init__(self, key=None):\n",
    "        \"\"\"\n",
    "        Initializes the articleAPI class with a developer key. Raises an exception if a key is not given.\n",
    "        Request a key at http://developer.nytimes.com/docs/reference/keys\n",
    "        :param key: New York Times Developer Key\n",
    "        \"\"\"\n",
    "        self.key = key\n",
    "        self.response_format = 'json'\n",
    "\n",
    "        if self.key is None:\n",
    "            raise NoAPIKeyException('Warning: Missing API Key. Please visit ' + API_SIGNUP_PAGE + ' to register for a key.')\n",
    "\n",
    "    def _bool_encode(self, d):\n",
    "        \"\"\"\n",
    "        Converts bool values to lowercase strings\n",
    "        \"\"\"\n",
    "        for k, v in d.items():\n",
    "            if isinstance(v, bool):\n",
    "                d[k] = str(v).lower()\n",
    "\n",
    "        return d\n",
    "\n",
    "    def _options(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Formats search parameters/values for use with API\n",
    "        :param \\*\\*kwargs: search parameters/values\n",
    "        \"\"\"\n",
    "        def _format_fq(d):\n",
    "            for k, v in d.items():\n",
    "                if isinstance(v, list):\n",
    "                    d[k] = ' '.join(map(lambda x: '\"' + x + '\"', v))\n",
    "                else:\n",
    "                    d[k] = '\"' + str(v) + '\"'\n",
    "            values = []\n",
    "            for k, v in d.items():\n",
    "                value = '%s:(%s)' % (k, v)\n",
    "                values.append(value)\n",
    "            values = ' AND '.join(values)\n",
    "            return values\n",
    "\n",
    "        kwargs = self._bool_encode(kwargs)\n",
    "\n",
    "        values = ''\n",
    "\n",
    "        for k, v in kwargs.items():\n",
    "            if k is 'fq' and isinstance(v, dict):\n",
    "                v = _format_fq(v)\n",
    "            elif isinstance(v, list):\n",
    "                v = ','.join(v)\n",
    "            values += '%s=%s&' % (k, v)\n",
    "\n",
    "        return values\n",
    "\n",
    "    def search(self,\n",
    "               response_format=None,\n",
    "               key=None,\n",
    "               **kwargs):\n",
    "        \"\"\"\n",
    "        Calls the API and returns a dictionary of the search results\n",
    "        :param response_format: the format that the API uses for its response,\n",
    "                                includes JSON (.json) and JSONP (.jsonp).\n",
    "                                Defaults to '.json'.\n",
    "        :param key: a developer key. Defaults to key given when the articleAPI class was initialized.\n",
    "        \"\"\"\n",
    "        if response_format is None:\n",
    "            response_format = self.response_format\n",
    "        if key is None:\n",
    "            key = self.key\n",
    "\n",
    "        url = '%s%s?%sapi-key=%s' % (\n",
    "            API_ROOT, response_format, self._options(**kwargs), key\n",
    "        )\n",
    "\n",
    "        r = requests.get(url)\n",
    "        return r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### User Input HERE\n",
    "YourAPI = '234342221c0c4f2fa969f69d92a6f700'\n",
    "\n",
    "## Article Search\n",
    "Start_Date = 20160101\n",
    "End_Date = 20170520\n",
    "Query_Phrase = 'Artificial Intelligence'\n",
    "\n",
    "##number of topics you want\n",
    "method = \"LDA\"\n",
    "num_of_topics = 10\n",
    "num_of_words = 15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "api = articleAPI(YourAPI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hideCode": false
   },
   "outputs": [],
   "source": [
    "def parse_articles(articles):\n",
    "    '''\n",
    "    This function takes in a response to the NYT api and parses\n",
    "    the articles into a list of dictionaries\n",
    "    '''\n",
    "    news = []\n",
    "    for i in articles[\"response\"]['docs']:\n",
    "        dic = {}\n",
    "        dic['id'] = i['_id']\n",
    "        if i['abstract'] is not None:\n",
    "            dic['abstract'] = i['abstract'].encode(\"utf8\")\n",
    "        dic['headline'] = i['headline']['main'].encode(\"utf8\")\n",
    "        dic['desk'] = i['news_desk']\n",
    "        dic['date'] = i['pub_date'][0:10] # cutting time of day.\n",
    "        dic['section'] = i['section_name']\n",
    "        if i['snippet'] is not None:\n",
    "            dic['snippet'] = i['snippet'].encode(\"utf8\")\n",
    "        dic['source'] = i['source']\n",
    "        dic['type'] = i['type_of_material']\n",
    "        dic['url'] = i['web_url']\n",
    "        dic['word_count'] = i['word_count']\n",
    "        # locations\n",
    "        locations = []\n",
    "        for x in range(0,len(i['keywords'])):\n",
    "            if 'glocations' in i['keywords'][x]['name']:\n",
    "                locations.append(i['keywords'][x]['value'])\n",
    "        dic['locations'] = locations\n",
    "        # subject\n",
    "        subjects = []\n",
    "        for x in range(0,len(i['keywords'])):\n",
    "            if 'subject' in i['keywords'][x]['name']:\n",
    "                subjects.append(i['keywords'][x]['value'])\n",
    "        dic['subjects'] = subjects   \n",
    "        news.append(dic)\n",
    "    return(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hideCode": false
   },
   "outputs": [],
   "source": [
    "def get_articles(begindate, enddate, query):\n",
    "    '''\n",
    "    This function needs to change  begin_date  and number of pages 10 artilces per page\n",
    "    '''\n",
    "    all_articles = []\n",
    "    for i in range(0, 20): #NYT limits pager to first 100 pages. But rarely will you find over 100 pages of results anyway.\n",
    "        articles = api.search(q = query,\n",
    "               fq = {'source':['Reuters','AP', 'The New York Times']},\n",
    "               begin_date = begindate,\n",
    "               end_date = enddate,\n",
    "               sort='newest',\n",
    "               page = str(i))\n",
    "        print (\"page\" + str(i), list(articles.keys()))\n",
    "        if list(articles.keys()) == ['message']:\n",
    "            articles = []\n",
    "        else:\n",
    "            articles = parse_articles(articles)\n",
    "        all_articles = all_articles + articles\n",
    "    return(all_articles)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GIVE ME A KEY WORD OR COMPANY NAME TO RUN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hideCode": true,
    "hideOutput": true,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(get_articles(20160101, 20170514, \"supply chain management\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hideCode": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hideCode": false
   },
   "outputs": [],
   "source": [
    "df.section = df[\"section\"].str.decode(\"utf-8\")\n",
    "df.headline =df[\"headline\"].str.decode(\"utf-8\")\n",
    "df.snippet = df[\"snippet\"].str.decode(\"utf-8\")\n",
    "df[\"headline+snippet\"] = df.headline.astype(str) + df.snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hideCode": false
   },
   "outputs": [],
   "source": [
    "##tdidf the word -rrix\n",
    "tf = TfidfVectorizer(analyzer='word', \n",
    "                     ngram_range=(1, 3), \n",
    "                     max_df = 0.95, \n",
    "                     min_df=0, \n",
    "                     stop_words='english')\n",
    "tfidf_matrix = tf.fit_transform(df[\"headline+snippet\"])\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "cosine_sim = cosine_distances(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "results = {}\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    similar_indices = cosine_sim[idx].argsort()[:-100:-1]\n",
    "    similar_items = [(cosine_sim[idx][i], df[\"headline+snippet\"][i]) for i in similar_indices]\n",
    "\n",
    "    # First item is the item itself, so remove it.\n",
    "    # Each dictionary entry is like: [(1,2), (3,4)], with each tuple being (score, item_id)\n",
    "    results[row['headline']] = similar_items[1:]\n",
    "    \n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rank the popularity of articles by the similarity among other artiles; in other words, this top ranks the stories by popularity in the news agency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##create a hot news table\n",
    "df[\"HeatLevel\"] = pd.Series()\n",
    "for i in range(len(df[\"headline\"])):\n",
    "    df[\"HeatLevel\"][i] = len([t for t in results[df[\"headline\"][i]] if t[0] > 0.05])   \n",
    "pd.options.display.max_colwidth = 100\n",
    "Ranking = df.sort(columns=\"HeatLevel\", axis=0, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Ranking.to_csv(\"~/Desktop/api_ranked.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What general topics are there among all the news and frequency of each modeling?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##input a documenation as df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hideCode": false
   },
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import sklearn.feature_extraction.text as text\n",
    "from sklearn import decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hideCode": false
   },
   "outputs": [],
   "source": [
    "##Non-negative Matrix Factorization, NMF method to surfac the topics; the reason is that \n",
    "\n",
    "def print_topic_words(method = \"NMF\", num_topics = 20, num_top_words = 15):\n",
    "    topic_words = []\n",
    "    if method == \"NMF\":\n",
    "        tf = TfidfVectorizer(analyzer='word', \n",
    "                     ngram_range=(1, 1), \n",
    "                     max_df = 0.95, \n",
    "                     min_df=0, \n",
    "                     stop_words='english')\n",
    "        tfidf = tf.fit_transform(df[\"headline+snippet\"].tolist()).toarray()\n",
    "        vocab = np.array(tf.get_feature_names())\n",
    "        clf = decomposition.NMF(n_components=num_topics, random_state=1, alpha=.1, l1_ratio=.5)\n",
    "        doctopic = clf.fit_transform(tfidf)\n",
    "    elif method == \"LDA\":\n",
    "        count = text.CountVectorizer(df[\"headline+snippet\"].tolist(), max_df = 0.95, \n",
    "                     min_df=0,stop_words='english')\n",
    "        dtm = count.fit_transform(df[\"headline+snippet\"].tolist()).toarray()\n",
    "        vocab = np.array(count.get_feature_names())\n",
    "        clf = decomposition.LatentDirichletAllocation(n_topics=num_topics, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=1)\n",
    "        doctopic = clf.fit_transform(dtm)\n",
    "    for topic in clf.components_:\n",
    "        word_idx = np.argsort(topic)[::-1][0:num_top_words]\n",
    "        topic_words.append([vocab[i] for i in word_idx])\n",
    "    return topic_words, doctopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topic_words, doctopic = print_topic_words(method = method, num_topics = num_of_topics, num_top_words = num_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(data= (doctopic / (np.sum(doctopic, axis=1, keepdims=True) + 1e-6) )* 100, index = df[\"headline\"] , columns = range(num_of_topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reordered = []\n",
    "doctopic_weights = np.sum(doctopic, axis=0)\n",
    "ranking = np.argsort(doctopic_weights)[::-1]  \n",
    "\n",
    "for i, x in enumerate(ranking):\n",
    "    reordered.append(topic_words[x])   \n",
    "    print(\"Topic {}: {}\".format(ranking[i], ' '.join(reordered[i][: num_of_words])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###put in our search words, if it's not in the topic, it will return error\n",
    "test = ['AI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doctopic = doctopic / np.sum(doctopic, axis=1, keepdims=True)\n",
    "for i in range(len(test)):\n",
    "    top_topics = np.argsort(doctopic[i,:])[::-1][:5]\n",
    "    top_topics_str = ' '.join(str(t) for t in top_topics)\n",
    "    print(\"{}: in topic {}\".format(test[i], top_topics_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is to recommend similar articles for you to read further!\n",
    "# Recommend(\"*headline of your artile*\", numbr of articles, you want)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Just reads the results out of the dictionary. \n",
    "def recommend(headline, num):\n",
    "    print(\"Recommending all related articles similar to \" + headline)\n",
    "    print(\"-------\")\n",
    "    recs = results[headline][:num]\n",
    "    for rec in recs:\n",
    "        print(\"Recommended: \" + rec[1] + \" (score:\" + str(rec[0]) + \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import ward, dendrogram\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "dist = 1- cosine_distances(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "\n",
    "linkage_matrix = ward(dist)\n",
    "\n",
    "plt.figure(figsize=(11.3, 11.3))  # we need a tall figure\n",
    "\n",
    "# match dendrogram to that returned by R's hclust()\n",
    "dendrogram(linkage_matrix, orientation=\"right\", leaf_font_size=5);\n",
    "plt.tight_layout()  # fixes margins"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Hide code",
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
