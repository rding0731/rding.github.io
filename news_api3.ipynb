{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "API_ROOT = 'http://api.nytimes.com/svc/search/v2/articlesearch.'\n",
    "\n",
    "API_SIGNUP_PAGE = 'http://developer.nytimes.com/docs/reference/keys'\n",
    "\n",
    "\n",
    "class NoAPIKeyException(Exception):\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "\n",
    "    def __str__(self):\n",
    "        return repr(self.value)\n",
    "\n",
    "\n",
    "class articleAPI(object):\n",
    "    def __init__(self, key=None):\n",
    "        \"\"\"\n",
    "        Initializes the articleAPI class with a developer key. Raises an exception if a key is not given.\n",
    "        Request a key at http://developer.nytimes.com/docs/reference/keys\n",
    "        :param key: New York Times Developer Key\n",
    "        \"\"\"\n",
    "        self.key = key\n",
    "        self.response_format = 'json'\n",
    "\n",
    "        if self.key is None:\n",
    "            raise NoAPIKeyException('Warning: Missing API Key. Please visit ' + API_SIGNUP_PAGE + ' to register for a key.')\n",
    "\n",
    "    def _bool_encode(self, d):\n",
    "        \"\"\"\n",
    "        Converts bool values to lowercase strings\n",
    "        \"\"\"\n",
    "        for k, v in d.items():\n",
    "            if isinstance(v, bool):\n",
    "                d[k] = str(v).lower()\n",
    "\n",
    "        return d\n",
    "\n",
    "    def _options(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Formats search parameters/values for use with API\n",
    "        :param \\*\\*kwargs: search parameters/values\n",
    "        \"\"\"\n",
    "        def _format_fq(d):\n",
    "            for k, v in d.items():\n",
    "                if isinstance(v, list):\n",
    "                    d[k] = ' '.join(map(lambda x: '\"' + x + '\"', v))\n",
    "                else:\n",
    "                    d[k] = '\"' + str(v) + '\"'\n",
    "            values = []\n",
    "            for k, v in d.items():\n",
    "                value = '%s:(%s)' % (k, v)\n",
    "                values.append(value)\n",
    "            values = ' AND '.join(values)\n",
    "            return values\n",
    "\n",
    "        kwargs = self._bool_encode(kwargs)\n",
    "\n",
    "        values = ''\n",
    "\n",
    "        for k, v in kwargs.items():\n",
    "            if k is 'fq' and isinstance(v, dict):\n",
    "                v = _format_fq(v)\n",
    "            elif isinstance(v, list):\n",
    "                v = ','.join(v)\n",
    "            values += '%s=%s&' % (k, v)\n",
    "\n",
    "        return values\n",
    "\n",
    "    def search(self,\n",
    "               response_format=None,\n",
    "               key=None,\n",
    "               **kwargs):\n",
    "        \"\"\"\n",
    "        Calls the API and returns a dictionary of the search results\n",
    "        :param response_format: the format that the API uses for its response,\n",
    "                                includes JSON (.json) and JSONP (.jsonp).\n",
    "                                Defaults to '.json'.\n",
    "        :param key: a developer key. Defaults to key given when the articleAPI class was initialized.\n",
    "        \"\"\"\n",
    "        if response_format is None:\n",
    "            response_format = self.response_format\n",
    "        if key is None:\n",
    "            key = self.key\n",
    "\n",
    "        url = '%s%s?%sapi-key=%s' % (\n",
    "            API_ROOT, response_format, self._options(**kwargs), key\n",
    "        )\n",
    "\n",
    "        r = requests.get(url)\n",
    "        return r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "api = articleAPI('234342221c0c4f2fa969f69d92a6f700')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_articles(articles):\n",
    "    '''\n",
    "    This function takes in a response to the NYT api and parses\n",
    "    the articles into a list of dictionaries\n",
    "    '''\n",
    "    news = []\n",
    "    for i in articles[\"response\"]['docs']:\n",
    "        dic = {}\n",
    "        dic['id'] = i['_id']\n",
    "        if i['abstract'] is not None:\n",
    "            dic['abstract'] = i['abstract'].encode(\"utf8\")\n",
    "        dic['headline'] = i['headline']['main'].encode(\"utf8\")\n",
    "        dic['desk'] = i['news_desk']\n",
    "        dic['date'] = i['pub_date'][0:10] # cutting time of day.\n",
    "        dic['section'] = i['section_name']\n",
    "        if i['snippet'] is not None:\n",
    "            dic['snippet'] = i['snippet'].encode(\"utf8\")\n",
    "        dic['source'] = i['source']\n",
    "        dic['type'] = i['type_of_material']\n",
    "        dic['url'] = i['web_url']\n",
    "        dic['word_count'] = i['word_count']\n",
    "        # locations\n",
    "        locations = []\n",
    "        for x in range(0,len(i['keywords'])):\n",
    "            if 'glocations' in i['keywords'][x]['name']:\n",
    "                locations.append(i['keywords'][x]['value'])\n",
    "        dic['locations'] = locations\n",
    "        # subject\n",
    "        subjects = []\n",
    "        for x in range(0,len(i['keywords'])):\n",
    "            if 'subject' in i['keywords'][x]['name']:\n",
    "                subjects.append(i['keywords'][x]['value'])\n",
    "        dic['subjects'] = subjects   \n",
    "        news.append(dic)\n",
    "    return(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_articles(begindate, enddate, query):\n",
    "    '''\n",
    "    This function needs to change  begin_date  and number of pages 10 artilces per page\n",
    "    '''\n",
    "    all_articles = []\n",
    "    for i in range(0, 20): #NYT limits pager to first 100 pages. But rarely will you find over 100 pages of results anyway.\n",
    "        articles = api.search(q = query,\n",
    "               fq = {'source':['Reuters','AP', 'The New York Times']},\n",
    "               begin_date = begindate,\n",
    "               end_date = enddate,\n",
    "               sort='newest',\n",
    "               page = str(i))\n",
    "        print (\"page\" + str(i), list(articles.keys()))\n",
    "        if list(articles.keys()) == ['message']:\n",
    "            articles = []\n",
    "        else:\n",
    "            articles = parse_articles(articles)\n",
    "        all_articles = all_articles + articles\n",
    "    return(all_articles)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GIVE ME A KEY WORD OR COMPANY NAME TO RUN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "UPSall = get_articles(20160101, 20170514, \"supply chain management\")\n",
    "len(UPSall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ups = pd.DataFrame(UPSall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ds = ups\n",
    "ds.section = ds[\"section\"].str.decode(\"utf-8\")\n",
    "ds.headline =ds[\"headline\"].str.decode(\"utf-8\")\n",
    "ds.snippet = ds[\"snippet\"].str.decode(\"utf-8\")\n",
    "ds[\"headline+snippet\"] = ds.headline.astype(str) + ds.snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer(analyzer='word', ngram_range=(1, 3), min_df=0, stop_words='english')\n",
    "tfidf_matrix = tf.fit_transform(ds[\"headline+snippet\"])\n",
    "\n",
    "cosine_similarities = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "results = {}\n",
    "\n",
    "for idx, row in ds.iterrows():\n",
    "    similar_indices = cosine_similarities[idx].argsort()[:-100:-1]\n",
    "    similar_items = [(cosine_similarities[idx][i], ds['headline'][i]) for i in similar_indices]\n",
    "\n",
    "    # First item is the item itself, so remove it.\n",
    "    # Each dictionary entry is like: [(1,2), (3,4)], with each tuple being (score, item_id)\n",
    "    results[row['headline']] = similar_items[1:]\n",
    "    \n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rank the popularity of articles by the similarity among other artiles; in other words, this top ranks the stories by popularity in the news agency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##create a hot news table\n",
    "ups[\"HeatLevel\"] = pd.Series()\n",
    "for i in range(len(ups[\"headline\"])):\n",
    "    ups[\"HeatLevel\"][i] = len([t for t in results[ups[\"headline\"][i]] if t[0] > 0.05])   \n",
    "pd.options.display.max_colwidth = 100\n",
    "Ranking = ups.sort(columns=\"HeatLevel\", axis=0, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Ranking.to_csv(\"ranked_ups.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What general topics are there among all the news and frequency of each modeling?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import sklearn.feature_extraction.text as text\n",
    "vectorizer = text.CountVectorizer(ds[\"headline+snippet\"].tolist(), stop_words='english')\n",
    "dtm = vectorizer.fit_transform(ds[\"headline+snippet\"].tolist()).toarray()\n",
    "vocab = np.array(vectorizer.get_feature_names())\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "def print_topic_words(num_topics, num_top_words):\n",
    "    topic_words = []\n",
    "    clf = decomposition.NMF(n_components=num_topics, random_state=1)\n",
    "    doctopic = clf.fit_transform(dtm)\n",
    "    for topic in clf.components_:\n",
    "        word_idx = np.argsort(topic)[::-1][0:num_top_words]\n",
    "        topic_words.append([vocab[i] for i in word_idx])\n",
    "    return topic_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## USER NEED TO SPEICIFIC PICK THE NUMBER OF TOPICS AND NUMBER OF TOP WORDS\n",
    "topic_words = print_topic_words(num_topics = 20, num_top_words = 15)\n",
    "topic_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_topics = 20\n",
    "clf = decomposition.NMF(n_components=num_topics, random_state=1)\n",
    "doctopic = clf.fit_transform(dtm)\n",
    "\n",
    "doctopic_orig = doctopic.copy()\n",
    "doctopic_orig = np.sum(doctopic, axis=0)\n",
    "ranking = np.argsort(doctopic_orig)[::-1]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reordered = []\n",
    "for i, x in enumerate(ranking):\n",
    "    reordered.append(topic_words[x])\n",
    "    \n",
    "for t in range(len(reordered)):     \n",
    "    print(\"Topic {}: {}\".format(t, ' '.join(reordered[t][:15])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "novel_names = np.asarray(ds[\"headline+snippet\"])\n",
    "doctopic_grouped = np.zeros((91, num_topics))\n",
    "for i, name in enumerate(sorted(set(novel_names))):\n",
    "    doctopic_grouped[i, :] = np.mean(doctopic[novel_names == name, :], axis=0)\n",
    "doctopic = doctopic_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###put in our search words, if it's not in the topic, it will return error\n",
    "test = [\"logistics\", \"fullfillment\", \"UPS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doctopic = doctopic / np.sum(doctopic, axis=1, keepdims=True)\n",
    "for i in range(len(test)):\n",
    "    top_topics = np.argsort(doctopic[i,:])[::-1][:3]\n",
    "    top_topics_str = ' '.join(str(t) for t in top_topics)\n",
    "    print(\"{}: in topic {}\".format(test[i], top_topics_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is to recommend similar articles for you to read further!\n",
    "# Recommend(\"*headline of your artile*\", numbr of articles, you want)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Just reads the results out of the dictionary. \n",
    "def recommend(headline, num):\n",
    "    print(\"Recommending all related articles similar to \" + headline)\n",
    "    print(\"-------\")\n",
    "    recs = results[headline][:num]\n",
    "    for rec in recs:\n",
    "        print(\"Recommended: \" + rec[1] + \" (score:\" + str(rec[0]) + \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
